![nlp](/images/part2_nlp.png)
### Модуль 1. Введение в NLP и эмбеддинги слов
Знакомство с базовыми понятиями обработки естественного языка и методами представления слов в виде эмбеддингов.
Домашнее задание: ранжирование текстов на основе эмбеддингов.

### Модуль 2. Рекуррентные нейронные сети (RNN)
Изучение рекуррентных архитектур (RNN, LSTM, GRU) и их применение для классификации текстов.
Домашнее задание: классификация текста с помощью RNN.

### Модуль 3. Языковое моделирование
Разбор классических и нейросетевых подходов к языковому моделированию, метрик качества и методов генерации текста. Практика по обучению языковой модели на уровне слов.
Домашнее задание: построение языковой модели на RNN.

### Модуль 4. Машинный перевод и Attention
Задача машинного перевода: от базовых архитектур к механизму Attention, который значительно улучшает качество перевода.

### Модуль 5. Transformer
Изучение архитектуры Transformer — модели, основанной на механизме внимания, которая произвела революцию в NLP и стала новым стандартом.

### Модуль 6. Предобучение и дообучение языковых моделей
Современные методы предобучения языковых моделей и практики fine-tuning для прикладных задач.
Домашнее задание: дообучение предобученной языковой модели для классификации.

### Модуль 7. От GPT до GPT-3. Zero-shot Learning
Эволюция GPT-подобных моделей: от первых версий до GPT-3. Обсуждение концепции Zero-shot Learning и её применения.

### Модуль 8. GPT-2, GPT-3 и Retrieval-Augmented Generation (RAG)
Продолжение изучения GPT-семейства моделей и знакомство с подходом RAG.
Домашнее задание: реализация RAG-модели.

### Модуль 9. Интерпретируемость трансформеров
Разбор подходов к интерпретации трансформеров: SAE, механизмы внимания, circuits и способы анализа работы модели.

### Модуль 10. Детекция сгенерированных текстов
Современные методы выявления текста, созданного нейросетевыми моделями, и связанные с этим вызовы.
Домашнее задание: детекция сгенерированных текстов.